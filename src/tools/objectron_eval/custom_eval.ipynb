{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 05:07:10.376065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-21 05:07:10.388332: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-21 05:07:10.391880: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-21 05:07:10.401609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-21 05:07:11.075112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import argparse\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial.transform import Rotation as rotation_util\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import objectron.dataset.iou as IoU3D\n",
    "import objectron.dataset.box as Box\n",
    "\n",
    "# We incorporate confidence into the AP calculation\n",
    "# METRIC_UPDATED = False\n",
    "METRIC_UPDATED = True\n",
    "if METRIC_UPDATED:\n",
    "    import objectron.dataset.metrics_nvidia as metrics\n",
    "else:\n",
    "    import objectron.dataset.metrics as metrics\n",
    "\n",
    "import objectron.dataset.parser as parser\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from lib.utils.pnp.cuboid_pnp_shell import pnp_shell\n",
    "\n",
    "import shutil\n",
    "import simplejson as json\n",
    "\n",
    "from lib.detectors.detector_factory import detector_factory\n",
    "from lib.opts import opts\n",
    "\n",
    "import cv2\n",
    "\n",
    "from eval_opts import eval_opts\n",
    "from eval_utils import draw_axes\n",
    "\n",
    "_MAX_PIXEL_ERROR = 0.1\n",
    "_MAX_AZIMUTH_ERROR = 30.\n",
    "_MAX_POLAR_ERROR = 20.\n",
    "_MAX_SCALE_ERROR = 2.\n",
    "_MAX_DISTANCE = 1.0  # In meters\n",
    "_NUM_BINS = 21\n",
    "\n",
    "dimension_ref = {\n",
    "    'bike': [[0.65320896, 1.021797894, 1.519635599, 0.6520559199, 1.506392621],\n",
    "             [0.1179380561, 0.176747817, 0.2981715678, 0.1667947895, 0.3830536275]],\n",
    "    'book': [[0.225618019, 0.03949624326, 0.1625821624, 7.021850281, 5.064694187],\n",
    "             [0.1687487664, 0.07391230822, 0.06436673199, 3.59629568, 2.723290812]],\n",
    "    'bottle': [\n",
    "        [0.07889784977450116, 0.24127451915330908, 0.0723714257114412, 0.33644069262302545, 0.3091134992864717, ],\n",
    "        [0.02984649578071775, 0.06381390122918497, 0.03088144838560917, 0.11052240441921059,\n",
    "         0.13327627592012867, ]],\n",
    "    'camera': [[0.11989848700326843, 0.08226238775595619, 0.09871718158089632, 1.507216484439368, 1.1569407159290284, ],\n",
    "               [0.021177290310316968, 0.02158788017191602, 0.055673710278419844, 0.28789183678046854,\n",
    "                0.5342094080365904, ]],\n",
    "    'cereal_box': [\n",
    "        [0.19202754401417296, 0.2593114001714919, 0.07723794925413519, 0.7542602699204104, 0.29441151268928173, ],\n",
    "        [0.08481640897407464, 0.09999915952084068, 0.09495429981036707, 0.19829004029411457, 0.2744797990483879, ]],\n",
    "    'chair': [[0.5740664085137888, 0.8434027515832329, 0.6051523831888338, 0.6949691013776601, 0.7326891354260606, ],\n",
    "              [0.12853104253707456, 0.14852086453095492, 0.13428881418587957, 0.16897092539619352,\n",
    "               0.18636134566748525, ]],\n",
    "    'cup': [[0.08587637391801063, 0.12025228955138188, 0.08486836104868696, 0.7812126934904675, 0.7697895244331658, ],\n",
    "            [0.05886805978497525, 0.06794896438246326, 0.05875681990718713, 0.2887038681446475, 0.283821205157399, ]],\n",
    "    'mug': [[0.14799136566553112, 0.09729087667918128, 0.08845449667169905, 1.3875694883045138, 1.0224997119392225, ],\n",
    "            [1.0488828523223728, 0.2552672927963539, 0.039095350310480705, 0.3947832854104711, 0.31089415283872546, ]],\n",
    "    'laptop': [[0.33685059747485196, 0.1528068814247063, 0.2781020624738614, 35.920214652427696, 23.941173992376903, ],\n",
    "               [0.03529983948867832, 0.07017080198389423, 0.0665823136876069, 391.915687801732, 254.21325950495455, ]],\n",
    "    'shoe': [[0.10308848289662519, 0.10932616184503478, 0.2611737789760352, 1.0301976264129833, 2.6157393112424328, ],\n",
    "             [0.02274768925924402, 0.044958380226590516, 0.04589720205423542, 0.3271000267177176,\n",
    "              0.8460337534776092, ]],\n",
    "}\n",
    "\n",
    "epnp_alpha_default = np.array([4.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0,\n",
    "                               -1.0, 1.0, -1.0, 0.0, -1.0, 1.0, 1.0, 2.0, 1.0, -1.0, -1.0,\n",
    "                               0.0, 1.0, -1.0, 1.0, 0.0, 1.0, 1.0, -1.0, -2.0, 1.0, 1.0,\n",
    "                               1.0]).reshape(8, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 경로 txt 파일생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = '/home/custom_dataset/box_folders'\n",
    "# output_file = base_dir + '/image_paths.txt'\n",
    "\n",
    "# with open(output_file, 'w') as f:\n",
    "#     for root, dirs, files in tqdm.tqdm(os.walk(base_dir)):\n",
    "#         if '_output' in root:\n",
    "#             for file in files:\n",
    "#                 if file.endswith('.jpg'):\n",
    "#                     full_path = os.path.join(root, file)\n",
    "#                     f.write(full_path + '\\n')\n",
    "\n",
    "# print(f'이미지 경로가 {output_file}에 저장되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 275개의 이미지 경로를 읽었습니다.\n",
      "첫 번째 이미지 경로: /home/custom_dataset/box_folders/box_tea_1/_output/0005.jpg\n",
      "마지막 이미지 경로: /home/custom_dataset/box_folders/box_tissue_2/_output/0001.jpg\n"
     ]
    }
   ],
   "source": [
    "# 이미지 경로가 저장된 txt 파일 읽기\n",
    "image_paths_file = '/home/custom_dataset/box_folders/image_paths.txt'\n",
    "\n",
    "# 빈 리스트 생성\n",
    "image_paths = []\n",
    "\n",
    "# txt 파일을 열어서 각 줄을 리스트에 추가\n",
    "with open(image_paths_file, 'r') as f:\n",
    "    for line in f:\n",
    "        image_paths.append(line.strip())\n",
    "\n",
    "print(f'총 {len(image_paths)}개의 이미지 경로를 읽었습니다.')\n",
    "print(f'첫 번째 이미지 경로: {image_paths[0]}')\n",
    "print(f'마지막 이미지 경로: {image_paths[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자체 Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        json_path = image_path.replace('.jpg', '.json')\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            xy_dict = json_data['labelingInfo'][0]['3DBox']['location'][0]\n",
    "            label = [[int(xy_dict[f\"x{i}\"]), int(xy_dict[f\"y{i}\"])] for i in [9]+list(range(1, 9))]\n",
    "\n",
    "        return image_path, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1237, 559], [1086, 551], [1344, 599], [1332, 687], [1086, 633], [1160, 431], [1389, 464], [1385, 533], [1158, 494]]\n"
     ]
    }
   ],
   "source": [
    "my_dataset = CustomDataset(image_paths)\n",
    "coords_2d = my_dataset[0][1]\n",
    "print(coords_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default param setting for opt_eval\n",
    "opt_eval = eval_opts().parser.parse_args([])\n",
    "\n",
    "# Default param setting for opt_detector\n",
    "opt_detector = opts().parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Some important settings to change, should be commented if not using Pycharm but .sh instead\n",
    "opt_eval.outf = 'custom_eval/'\n",
    "# opt_eval.eval_num_symmetry = 1\n",
    "# opt_eval.eval_confidence_thresh=0.3\n",
    "opt_eval.eval_debug_save_thresh = 100\n",
    "opt_eval.eval_max_num = len(my_dataset)\n",
    "opt_detector.nms = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the subset, default is False\n",
    "if opt_eval.eval_subset == True:\n",
    "    json_path = 'selected_frames.json'\n",
    "    with open(json_path) as f:\n",
    "        data_json = json.load(f)\n",
    "    opt_eval.eval_subset_list = list(map(int, data_json[opt_eval.eval_c]))\n",
    "    opt_eval.eval_subset_list.sort()\n",
    "else:\n",
    "    opt_eval.eval_subset_list = None\n",
    "\n",
    "# For debug purpose\n",
    "# opt_eval.eval_skip=35 # Run images on [eval_skip, eval_max_num]\n",
    "# opt_detector.batch_size=1\n",
    "# opt_eval.eval_debug = True # Whether to save img for debug\n",
    "opt_eval.eval_debug_json = True  # Whether to save json for debug\n",
    "opt_eval.eval_debug_clean = True\n",
    "\n",
    "opt_detector.debug = 0  # do not save extra visualization in demo/ for debug, e.g., heatmap\n",
    "# opt_detector.debug = 6  # save extra visualization in demo/ for debug, e.g., heatmap\n",
    "\n",
    "# Objectron paper https://arxiv.org/abs/2012.09988 assumes mug is also symmetric, for fair comparison we also have this option\n",
    "opt_eval.eval_mug_symmetric = True\n",
    "\n",
    "# True: only evaluate mug case, False: only evaluate cup case, None: Evaluate them all\n",
    "opt_eval.mug_only = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_eval.eval_c = 'cereal_box' # default 'bike'\n",
    "opt_eval.eval_c = 'chair'\n",
    "\n",
    "opt_detector.c = opt_eval.eval_c\n",
    "\n",
    "opt_eval.eval_arch = 'dlav1_34' # default 'dla_34'\n",
    "opt_detector.arch = opt_eval.eval_arch\n",
    "\n",
    "opt_detector.rep_mode = opt_eval.eval_rep_mode\n",
    "opt_detector.vis_thresh = opt_eval.eval_confidence_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No symmetry\n",
    "if 'v1' in opt_detector.arch:\n",
    "    opt_eval.report_file = f'{opt_detector.c}_v1_report_{opt_eval.eval_confidence_thresh}.txt'\n",
    "    opt_detector.load_model = f\"../../../models/CenterPose/{opt_detector.c}_v1_{opt_eval.eval_weight_id}.pth\"\n",
    "else:\n",
    "    opt_eval.report_file = f'{opt_detector.c}_report_{opt_eval.eval_confidence_thresh}.txt'\n",
    "    opt_detector.load_model = f\"../../../models/CenterPose/{opt_detector.c}_{opt_eval.eval_weight_id}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(opt_eval.eval_save_id)\n",
    "print(opt_eval.eval_rep_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder custom_eval// exists\n",
      "created folder custom_eval//chair_1\n",
      "Fix size testing.\n",
      "training chunk_sizes: [32]\n",
      "The output will be saved to  /home/CenterPose/src/tools/objectron_eval/../../lib/../../exp/object_pose/default\n",
      "heads {'hm': 1, 'wh': 2, 'hps': 16, 'reg': 2, 'hm_hp': 8, 'hp_offset': 2, 'scale': 3}\n"
     ]
    }
   ],
   "source": [
    "# Some exp naming rules\n",
    "opt_eval.eval_save_id = opt_eval.eval_rep_mode\n",
    "if opt_detector.nms == True:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_nms.txt'\n",
    "if opt_detector.rep_mode == 0:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_8rep.txt'\n",
    "elif opt_detector.rep_mode == 1:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_16rep.txt'\n",
    "elif opt_detector.rep_mode == 2:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_samplerep.txt'\n",
    "elif opt_detector.rep_mode == 3:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_disrep.txt'\n",
    "elif opt_detector.rep_mode == 4:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_hmrep.txt'\n",
    "\n",
    "if opt_eval.eval_arch == 'dla_34':\n",
    "    opt_eval.eval_save_id = 5\n",
    "if opt_eval.eval_MobilePose_postprocessing == True:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_MobilePose.txt'\n",
    "    opt_eval.eval_save_id = 6\n",
    "if opt_eval.eval_gt_scale == True:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_gtscale.txt'\n",
    "    opt_eval.eval_save_id = 7\n",
    "if opt_eval.eval_mug_symmetric == False:\n",
    "    opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + '_partsymmetry.txt'\n",
    "    opt_eval.eval_save_id = 8\n",
    "\n",
    "opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + f'_{opt_eval.eval_max_num}.txt'\n",
    "opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + f'_{opt_eval.eval_exp_id}.txt'\n",
    "opt_eval.report_file = os.path.splitext(opt_eval.report_file)[0] + f'_{opt_eval.eval_weight_id}.txt'\n",
    "\n",
    "if opt_eval.eval_debug == True or opt_eval.eval_debug_json == True:\n",
    "    if opt_eval.eval_debug_clean == True and opt_eval.eval_continue != True:\n",
    "        # Clean up debug/\n",
    "        if os.path.isdir(f'{opt_eval.outf}/{opt_detector.c}_{opt_eval.eval_save_id}'):\n",
    "            shutil.rmtree(f'{opt_eval.outf}/{opt_detector.c}_{opt_eval.eval_save_id}')\n",
    "        # Clean up demo/\n",
    "        if os.path.exists(\n",
    "                os.path.join('demo/', f'{os.path.splitext(os.path.basename(opt_detector.load_model))[0]}')):\n",
    "            shutil.rmtree(\n",
    "                os.path.join('demo/', f'{os.path.splitext(os.path.basename(opt_detector.load_model))[0]}'))\n",
    "\n",
    "    if os.path.isdir(f'{opt_eval.outf}'):\n",
    "        print(f'folder {opt_eval.outf}/ exists')\n",
    "    else:\n",
    "        os.mkdir(f'{opt_eval.outf}')\n",
    "        print(f'created folder {opt_eval.outf}/')\n",
    "\n",
    "    if os.path.isdir(f'{opt_eval.outf}/{opt_detector.c}_{opt_eval.eval_save_id}'):\n",
    "        print(f'folder {opt_eval.outf}/{opt_detector.c}_{opt_eval.eval_save_id} exists')\n",
    "    else:\n",
    "        os.mkdir(f'{opt_eval.outf}/{opt_detector.c}_{opt_eval.eval_save_id}')\n",
    "        print(f'created folder {opt_eval.outf}/{opt_detector.c}_{opt_eval.eval_save_id}')\n",
    "\n",
    "opt_detector.obj_scale = True\n",
    "opt_detector.use_pnp = True\n",
    "\n",
    "opt_detector = opts().parse(opt_detector)\n",
    "opt_detector = opts().init(opt_detector)\n",
    "opt_detector.eval_data = f'/{opt_detector.c}/{opt_detector.c}_test*'\n",
    "\n",
    "opt_combined = argparse.Namespace(**vars(opt_eval), **vars(opt_detector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "K: 100\n",
      "KL_kps_uncertainty: 0.1\n",
      "KL_scale_uncertainty: 0.1\n",
      "R: 20\n",
      "arch: dlav1_34\n",
      "aug_rot: 0\n",
      "balance_coefficient: {'bike': 2, 'book': 2, 'bottle': 2, 'camera': 2, 'cereal_box': 2, 'chair': 2, 'cup': 2, 'mug': 2, 'laptop': 2, 'shoe': 2}\n",
      "batch_size: 32\n",
      "c: chair\n",
      "cam_intrinsic: None\n",
      "center_3D: False\n",
      "center_thresh: 0.3\n",
      "chunk_sizes: [32]\n",
      "conf_border: {'bike': [3, 9], 'book': [3, 9], 'bottle': [3, 9], 'camera': [3, 9], 'cereal_box': [3, 9], 'chair': [3, 9], 'cup': [3, 9], 'mug': [3, 9], 'laptop': [3, 9], 'shoe': [3, 9]}\n",
      "data_dir: /home/CenterPose/src/tools/objectron_eval/../../lib/../../data\n",
      "data_generation_mode_ratio: 0\n",
      "dataset: objectron\n",
      "debug: 0\n",
      "debug_dir: /home/CenterPose/src/tools/objectron_eval/../../lib/../../exp/object_pose/default/debug\n",
      "debugger_theme: white\n",
      "demo: \n",
      "demo_save: ../demo/\n",
      "dense_hp: False\n",
      "down_ratio: 4\n",
      "empty_pre_hm: False\n",
      "eval_CenterPose_initialization: False\n",
      "eval_MobilePose_postprocessing: False\n",
      "eval_R: 20\n",
      "eval_add_noise: False\n",
      "eval_arch: dlav1_34\n",
      "eval_c: chair\n",
      "eval_confidence_thresh: 0.3\n",
      "eval_consistency_local_window: 5\n",
      "eval_continue: False\n",
      "eval_data: /chair/chair_test*\n",
      "eval_debug: False\n",
      "eval_debug_clean: True\n",
      "eval_debug_display: False\n",
      "eval_debug_json: True\n",
      "eval_debug_save_thresh: 100\n",
      "eval_empty_pre_hm: False\n",
      "eval_exp_id: 6\n",
      "eval_fake_output: False\n",
      "eval_gt_pre_hm_hmhp_first: False\n",
      "eval_gt_scale: False\n",
      "eval_hard_case: 0\n",
      "eval_hard_case_list: []\n",
      "eval_kalman: False\n",
      "eval_max_num: 275\n",
      "eval_mug_symmetric: True\n",
      "eval_noise_rot: 5\n",
      "eval_noise_scale: 20\n",
      "eval_noise_translation: 0.03\n",
      "eval_num_symmetry: 1\n",
      "eval_oracle_dep: False\n",
      "eval_oracle_hm: False\n",
      "eval_oracle_hmhp: False\n",
      "eval_oracle_hp_offset: False\n",
      "eval_oracle_kps: False\n",
      "eval_oracle_offset: False\n",
      "eval_oracle_wh: False\n",
      "eval_pre_hm: False\n",
      "eval_pre_hm_hp: False\n",
      "eval_refined_Kalman: False\n",
      "eval_rep_mode: 1\n",
      "eval_resolution_ratio: 2.4\n",
      "eval_save_id: 1\n",
      "eval_scale_pool: False\n",
      "eval_skip: 0\n",
      "eval_subset: False\n",
      "eval_subset_list: None\n",
      "eval_tracking_task: False\n",
      "eval_use_absolute_scale: False\n",
      "eval_weight_id: 140\n",
      "exp_dir: /home/CenterPose/src/tools/objectron_eval/../../lib/../../exp/object_pose\n",
      "exp_id: default\n",
      "fix_res: True\n",
      "fix_short: -1\n",
      "flip: 0.5\n",
      "flip_idx: [[1, 5], [3, 7], [2, 6], [4, 8]]\n",
      "fp_disturb: 0\n",
      "gpus: [0]\n",
      "gpus_str: 0\n",
      "gt_pre_hm_hmhp: False\n",
      "gt_pre_hm_hmhp_first: False\n",
      "head_conv: 256\n",
      "heads: {'hm': 1, 'wh': 2, 'hps': 16, 'reg': 2, 'hm_hp': 8, 'hp_offset': 2, 'scale': 3}\n",
      "hide_data_time: False\n",
      "hm_disturb: 0\n",
      "hm_heat_random: False\n",
      "hm_hp: True\n",
      "hm_hp_disturb: 0\n",
      "hm_hp_heat_random: False\n",
      "hm_hp_weight: 1\n",
      "hm_weight: 1\n",
      "hp_fp_disturb: 0\n",
      "hp_lost_disturb: 0\n",
      "hp_weight: 1\n",
      "hps_uncertainty: False\n",
      "hungarian: False\n",
      "input_h: 512\n",
      "input_res: 512\n",
      "input_w: 512\n",
      "kalman: False\n",
      "keep_res: False\n",
      "load_model: ../../../models/CenterPose/chair_v1_140.pth\n",
      "lost_disturb: 0\n",
      "lr: 0.000125\n",
      "lr_step: [90, 120]\n",
      "master_batch_size: 32\n",
      "max_age: 5\n",
      "max_frame_dist: 3\n",
      "mean: [0.408, 0.447, 0.47]\n",
      "metric: loss\n",
      "mse_loss: False\n",
      "mug: False\n",
      "mug_only: None\n",
      "new_data_augmentation: False\n",
      "new_thresh: 0.3\n",
      "nms: True\n",
      "no_color_aug: False\n",
      "not_cuda_benchmark: False\n",
      "not_hm_hp: False\n",
      "not_prefetch_test: False\n",
      "not_rand_crop: False\n",
      "not_reg_bbox: False\n",
      "not_reg_hp_offset: False\n",
      "not_reg_offset: False\n",
      "num_classes: 1\n",
      "num_epochs: 20\n",
      "num_iters: -1\n",
      "num_stacks: 1\n",
      "num_symmetry: 12\n",
      "num_workers: 4\n",
      "obj_scale: True\n",
      "obj_scale_uncertainty: False\n",
      "obj_scale_weight: 1\n",
      "off_weight: 1\n",
      "outf: custom_eval/\n",
      "output_h: 128\n",
      "output_res: 128\n",
      "output_w: 128\n",
      "pad: 31\n",
      "paper_display: False\n",
      "pre_hm: False\n",
      "pre_hm_hp: False\n",
      "pre_img: False\n",
      "pre_thresh: -1\n",
      "print_iter: 0\n",
      "refined_Kalman: False\n",
      "reg_bbox: True\n",
      "reg_hp_offset: True\n",
      "reg_loss: l1\n",
      "reg_offset: True\n",
      "render_hm_mode: 1\n",
      "render_hmhp_mode: 2\n",
      "rep_mode: 1\n",
      "report_file: chair_v1_report_0.3_nms_16rep_275_6_140.txt\n",
      "reportf: report\n",
      "resume: False\n",
      "root_dir: /home/CenterPose/src/tools/objectron_eval/../../lib/../..\n",
      "rotate: 0\n",
      "same_aug_pre: False\n",
      "save_all: False\n",
      "save_dir: /home/CenterPose/src/tools/objectron_eval/../../lib/../../exp/object_pose/default\n",
      "scale: 0.4\n",
      "scale_pool: False\n",
      "seed: 317\n",
      "shift: 0.05\n",
      "show_axes: False\n",
      "std: [0.289, 0.274, 0.278]\n",
      "task: object_pose\n",
      "test: False\n",
      "test_scales: [1.0]\n",
      "track_thresh: 0.3\n",
      "tracking: False\n",
      "tracking_hp: False\n",
      "tracking_hp_weight: 0.5\n",
      "tracking_label_mode: 1\n",
      "tracking_task: False\n",
      "tracking_weight: 1\n",
      "trainval: False\n",
      "use_absolute_scale: False\n",
      "use_pnp: True\n",
      "use_residual: False\n",
      "val_intervals: 5\n",
      "vis_thresh: 0.3\n",
      "wh_weight: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(len(vars(opt_combined)))\n",
    "# 각 속성과 값을 키의 사전순으로 출력\n",
    "for attr, value in sorted(vars(opt_combined).items()):\n",
    "    print(f\"{attr}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.debugger import Debugger\n",
    "from lib.detectors.object_pose import ObjectPoseDetector\n",
    "from lib.utils.pnp.cuboid_pnp_shell import pnp_shell_alter\n",
    "class MyObjectPoseDetector(ObjectPoseDetector):\n",
    "    def __init__(self, opt):\n",
    "        super().__init__(opt)\n",
    "\n",
    "    def run(self, image_or_path_or_tensor, filename=None, meta_inp={}, preprocessed_flag=False):\n",
    "        load_time, pre_time, net_time, dec_time, post_time = 0, 0, 0, 0, 0\n",
    "        merge_time, track_time, pnp_time, tot_time = 0, 0, 0, 0\n",
    "        debugger = Debugger(dataset=self.opt.dataset, ipynb=(self.opt.debug == 3),\n",
    "                            theme=self.opt.debugger_theme)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # pre_processed = False\n",
    "        pre_processed = preprocessed_flag\n",
    "\n",
    "        # File input\n",
    "        if isinstance(image_or_path_or_tensor, np.ndarray):\n",
    "            # For eval or for CenterPose as data generator\n",
    "            image = image_or_path_or_tensor\n",
    "\n",
    "            # We usually use image_or_path_or_tensor to represent filename\n",
    "            if filename is not None:\n",
    "                image_or_path_or_tensor = filename\n",
    "\n",
    "        # String input\n",
    "        elif type(image_or_path_or_tensor) == type(''):\n",
    "            # For demo\n",
    "            image = cv2.imread(image_or_path_or_tensor)\n",
    "        else:\n",
    "            # Not used yet\n",
    "            image = image_or_path_or_tensor['image'][0].numpy()\n",
    "            pre_processed_images = image_or_path_or_tensor\n",
    "            pre_processed = True\n",
    "\n",
    "        loaded_time = time.time()\n",
    "        load_time += (loaded_time - start_time)\n",
    "\n",
    "        detections = []\n",
    "        for scale in self.scales:\n",
    "            scale_start_time = time.time()\n",
    "            if not pre_processed:\n",
    "                images, meta = self.pre_process(image, scale, meta_inp)\n",
    "            else:\n",
    "\n",
    "                # Used for data generation\n",
    "                # 1 * 3 * 512 * 512\n",
    "                images = np.expand_dims(image, axis=0)\n",
    "                # images = image.reshape(1, 3, meta_inp['inp_height'], meta_inp['inp_width'])\n",
    "                images = torch.from_numpy(images)\n",
    "                meta = meta_inp\n",
    "\n",
    "            images = images.to(self.opt.device)\n",
    "\n",
    "            # initializing tracker\n",
    "            pre_hms, pre_hm_hp, pre_inds = None, None, None\n",
    "\n",
    "            if self.opt.refined_Kalman:\n",
    "                self.tracker.init_track(\n",
    "                    meta)\n",
    "\n",
    "            if self.opt.tracking_task:\n",
    "                # initialize the first frame\n",
    "                if self.pre_images is None:\n",
    "                    print('Initialize tracking!')\n",
    "                    self.pre_images = images\n",
    "                    self.tracker.init_track(\n",
    "                        meta)\n",
    "\n",
    "                # Initialize if given gt_pre_hm_hmhp\n",
    "                elif self.opt.gt_pre_hm_hmhp or (self.opt.gt_pre_hm_hmhp_first and meta['id'] == 0):\n",
    "                    self.tracker.init_track(\n",
    "                        meta)\n",
    "\n",
    "                if self.opt.pre_hm or self.opt.pre_hm_hp:\n",
    "                    # render input heatmap from tracker status\n",
    "                    # pre_inds is not used in the current version.\n",
    "                    # We used pre_inds for learning an offset from previous image to\n",
    "                    # the current image.\n",
    "                    pre_hms, pre_hm_hp, pre_inds = self._get_additional_inputs(\n",
    "                        self.tracker.tracks, meta, with_hm=self.opt.pre_hm, with_hm_hp=self.opt.pre_hm_hp)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            pre_process_time = time.time()\n",
    "            pre_time += pre_process_time - scale_start_time\n",
    "\n",
    "            # run the network\n",
    "            # output: the output feature maps, only used for visualizing\n",
    "            # dets: output tensors after extracting peaks\n",
    "            output, dets, forward_time = self.process(\n",
    "                images, self.pre_images, pre_hms, pre_hm_hp, pre_inds, return_time=True)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            net_time += forward_time - pre_process_time\n",
    "            decode_time = time.time()\n",
    "            dec_time += decode_time - forward_time\n",
    "\n",
    "            if self.opt.debug >= 2:\n",
    "                # Mainly save keypoint heatmap & displacement for debug\n",
    "                self.debug(debugger, images, copy.deepcopy(dets), output, scale, pre_hms, pre_hm_hp)\n",
    "\n",
    "            # convert the cropped and 4x downsampled output coordinate system\n",
    "            # back to the input image coordinate system\n",
    "            dets = self.post_process(dets, meta, scale)\n",
    "            torch.cuda.synchronize()\n",
    "            post_process_time = time.time()\n",
    "            post_time += post_process_time - decode_time\n",
    "\n",
    "            detections.append(dets)\n",
    "\n",
    "        # Mainly apply NMS\n",
    "        results = self.merge_outputs(detections)\n",
    "        torch.cuda.synchronize()\n",
    "        merge_outputs_time = time.time()\n",
    "        merge_time += merge_outputs_time - post_process_time\n",
    "\n",
    "        # print(\"results(detector.run().merge_outputs(), after nms) : \", )\n",
    "        # print(\"len(results) : \", len(results))\n",
    "        # pprint.pprint(results[0])\n",
    "\n",
    "\n",
    "        # The goal is to get 2d projection of keypoints & 6-DoF & 3d keypoint in camera frame\n",
    "        boxes = []\n",
    "        if self.opt.use_pnp == True:\n",
    "\n",
    "            for bbox in results:\n",
    "                # Point processing according to different rep_modes\n",
    "                if self.opt.rep_mode == 0 or self.opt.rep_mode == 3 or self.opt.rep_mode == 4:\n",
    "\n",
    "                    # 8 representation from centernet\n",
    "                    points = [(x[0], x[1]) for x in np.array(bbox['kps']).reshape(-1, 2)]\n",
    "                    points_filtered = points\n",
    "\n",
    "                elif self.opt.rep_mode == 1:\n",
    "\n",
    "                    # 16 representation\n",
    "                    points_1 = np.array(bbox['kps_displacement_mean']).reshape(-1, 2)\n",
    "                    points_1 = [(x[0] / meta['width'], x[1] / meta['height']) for x in points_1] \n",
    "                    \n",
    "                    points_2 = np.array(bbox['kps_heatmap_mean']).reshape(-1, 2)\n",
    "                    points_2 = [(x[0], x[1]) for x in points_2]\n",
    "                    \n",
    "                    points = np.hstack((points_1, points_2)).reshape(-1, 2)\n",
    "                    points_filtered = points\n",
    "\n",
    "                elif self.opt.rep_mode == 2:\n",
    "\n",
    "                    points = []\n",
    "\n",
    "                    N_sample = 20\n",
    "\n",
    "                    confidence_list = []\n",
    "                    dis_list = []\n",
    "                    weight_list = []\n",
    "\n",
    "                    keypoint_heatmap_mean_list = []\n",
    "                    keypoint_heatmap_std_list = []\n",
    "\n",
    "                    keypoint_displacement_mean_list = []\n",
    "                    keypoint_displacement_std_list = []\n",
    "\n",
    "                    GMM_list = []\n",
    "\n",
    "                    for i in range(8):\n",
    "\n",
    "                        # Normalized L2\n",
    "                        keypoint_displacement_norm = np.array(\n",
    "                            [bbox['kps_displacement_mean'][i * 2] / meta['width'],\n",
    "                             bbox['kps_displacement_mean'][i * 2 + 1] / meta['height']])\n",
    "                        keypoint_heatmap_norm = np.array(\n",
    "                            [bbox['kps_heatmap_mean'][i * 2] / meta['width'],\n",
    "                             bbox['kps_heatmap_mean'][i * 2 + 1] / meta['height']])\n",
    "                        dis = np.linalg.norm(keypoint_displacement_norm - keypoint_heatmap_norm)\n",
    "\n",
    "                        confidence_list.append(bbox['kps_heatmap_height'][i])\n",
    "                        dis_list.append(dis)\n",
    "\n",
    "                        def gaussian(dist, sigma=10.):\n",
    "                            return math.e ** (-dist ** 2 / 2 / sigma ** 2)\n",
    "\n",
    "                        # Calculate new weight list according to confidence & gaussian distribution on dis\n",
    "                        weight_list.append(confidence_list[i] * gaussian(dis))\n",
    "\n",
    "                        # 1. Heatmap\n",
    "                        keypoint_heatmap_mean = [bbox['kps_heatmap_mean'][i * 2], bbox['kps_heatmap_mean'][i * 2 + 1]]\n",
    "                        keypoint_heatmap_std = [bbox['kps_heatmap_std'][i * 2], bbox['kps_heatmap_std'][i * 2 + 1]]\n",
    "\n",
    "                        # 2. Displacement\n",
    "                        kps_displacement_mean = [bbox['kps_displacement_mean'][i * 2],\n",
    "                                                 bbox['kps_displacement_mean'][i * 2 + 1]]\n",
    "                        kps_displacement_std = keypoint_heatmap_std\n",
    "\n",
    "                        # Fit a GMM by sampling from keypoint_displacement &  keypoint_heatmap distributions\n",
    "                        X_train = []\n",
    "                        if keypoint_heatmap_mean[0] < -5000 or keypoint_heatmap_mean[1] < -5000:\n",
    "                            kps_displacement_std = [5, 5]\n",
    "                            points_sample = np.random.multivariate_normal(\n",
    "                                np.array(kps_displacement_mean),\n",
    "                                np.array([[kps_displacement_std[0], 0], [0, kps_displacement_std[1]]]), size=1000)\n",
    "                            X_train.append(points_sample)\n",
    "                        else:\n",
    "                            points_sample = np.random.multivariate_normal(\n",
    "                                np.array(keypoint_heatmap_mean),\n",
    "                                np.array([[keypoint_heatmap_mean[0], 0], [0, keypoint_heatmap_mean[1]]]), size=500)\n",
    "                            X_train.append(points_sample)\n",
    "\n",
    "                            points_sample = np.random.multivariate_normal(\n",
    "                                np.array(kps_displacement_mean),\n",
    "                                np.array([[kps_displacement_std[0], 0], [0, kps_displacement_std[1]]]), size=500)\n",
    "                            X_train.append(points_sample)\n",
    "\n",
    "                        keypoint_heatmap_mean_list.append(keypoint_heatmap_mean)\n",
    "                        keypoint_heatmap_std_list.append(keypoint_heatmap_std)\n",
    "                        keypoint_displacement_mean_list.append(kps_displacement_mean)\n",
    "                        keypoint_displacement_std_list.append(kps_displacement_std)\n",
    "\n",
    "                        X_train = np.array(X_train).reshape(-1, 2)\n",
    "                        clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "                        clf.fit(X_train)\n",
    "                        GMM_list.append(clf)\n",
    "\n",
    "                        points_sample = clf.sample(N_sample)\n",
    "                        points_sample = np.hstack((points_sample[0], np.array(points_sample[1]).reshape(-1, 1)))\n",
    "                        points.append(points_sample)\n",
    "\n",
    "                    points = np.array(points).reshape(-1, 3)\n",
    "                    # Do not need labels for pnp\n",
    "                    points_filtered = points[:, 0:2]\n",
    "\n",
    "                # ret = pnp_shell(self.opt, meta, bbox, points_filtered, bbox['obj_scale'], OPENCV_RETURN=self.opt.show_axes)\n",
    "                ret = pnp_shell_alter(self.opt, meta, bbox, points_filtered, bbox['obj_scale'], OPENCV_RETURN=self.opt.show_axes)\n",
    "\n",
    "                if ret is not None:\n",
    "                    boxes.append(ret)\n",
    "\n",
    "        pnp_process_time = time.time()\n",
    "        pnp_time += pnp_process_time - merge_outputs_time\n",
    "\n",
    "        # Tracker update\n",
    "        if self.opt.tracking_task:\n",
    "            results, boxes = self.tracker.step(results, boxes)\n",
    "            self.pre_images = images\n",
    "        # For baseline (CenterPose + Kalman)\n",
    "        elif self.opt.refined_Kalman:\n",
    "            results, boxes = self.tracker.step(results, boxes)\n",
    "\n",
    "        end_time = time.time()\n",
    "        track_time += end_time - pnp_process_time\n",
    "        tot_time += end_time - start_time\n",
    "\n",
    "        # Dict is for output debug\n",
    "        dict_out = {\"camera_data\": [], \"objects\": []}\n",
    "        if 'camera_matrix' in meta:\n",
    "            camera_matrix = meta['camera_matrix']\n",
    "            dict_out['camera_data'] = camera_matrix.tolist()\n",
    "\n",
    "        # if self.opt.tracking_task or self.opt.refined_Kalman:\n",
    "        #     for track in self.tracker.tracks:\n",
    "        #         # Basic part\n",
    "        #         dict_obj = {\n",
    "        #             'class': self.opt.c,\n",
    "        #             'ct': track['ct'],\n",
    "        #             'bbox': np.array(track['bbox']).tolist(),\n",
    "        #             'confidence': track['score'],\n",
    "        #             'kps_displacement_mean': track['kps_displacement_mean'].tolist(),\n",
    "        #             'kps_heatmap_mean': track['kps_heatmap_mean'].tolist(),\n",
    "\n",
    "        #             'kps_heatmap_std': track['kps_heatmap_std'].tolist(),\n",
    "        #             'kps_heatmap_height': track['kps_heatmap_height'].tolist(),\n",
    "        #             'obj_scale': (track['obj_scale']/track['obj_scale'][1]).tolist(),\n",
    "\n",
    "        #             'tracking_id':track['tracking_id'],\n",
    "        #         }\n",
    "\n",
    "        #         # Optional part\n",
    "        #         if self.opt.use_pnp:\n",
    "        #             if 'location' in track:\n",
    "        #                 dict_obj['location'] = track['location']\n",
    "        #                 dict_obj['quaternion_xyzw'] = track['quaternion_xyzw'].tolist()\n",
    "        #             if 'kps_pnp' in track:\n",
    "        #                 dict_obj['kps_pnp'] = track['kps_pnp'].tolist()\n",
    "        #                 dict_obj['kps_3d_cam'] = track['kps_3d_cam'].tolist()\n",
    "\n",
    "        #         if self.opt.obj_scale_uncertainty:\n",
    "        #             dict_obj['obj_scale_uncertainty'] = track['obj_scale_uncertainty'].tolist()\n",
    "\n",
    "        #         if self.opt.kalman:\n",
    "        #             dict_obj['kps_mean_kf'] = track['kps_mean_kf'].tolist()\n",
    "        #             dict_obj['kps_std_kf'] = track['kps_std_kf']\n",
    "        #             if self.opt.use_pnp and 'kps_pnp_kf' in track:\n",
    "        #                 dict_obj['kps_pnp_kf'] = track['kps_pnp_kf'].tolist()\n",
    "        #                 dict_obj['kps_3d_cam_kf'] = track['kps_3d_cam_kf'].tolist()\n",
    "\n",
    "        #         if self.opt.scale_pool == True:\n",
    "        #             dict_obj['obj_scale_kf'] = (track['obj_scale_kf']/track['obj_scale_kf'][1]).tolist()\n",
    "        #             dict_obj['obj_scale_uncertainty_kf'] = track['obj_scale_uncertainty_kf'].tolist()\n",
    "\n",
    "        #         if self.opt.hps_uncertainty:\n",
    "        #             dict_obj['kps_displacement_std'] = track['kps_displacement_std'].tolist()\n",
    "        #             dict_obj['kps_fusion_mean'] = track['kps_fusion_mean'].tolist()\n",
    "        #             dict_obj['kps_fusion_std'] = track['kps_fusion_std'].tolist()\n",
    "\n",
    "        #         if self.opt.tracking:\n",
    "        #             dict_obj['tracking'] = track['tracking'].tolist()\n",
    "        #         if self.opt.tracking_hp:\n",
    "        #             dict_obj['tracking_hp'] = track['tracking_hp'].tolist()\n",
    "\n",
    "        #         dict_out['objects'].append(dict_obj)\n",
    "        # else:\n",
    "        #     for box in boxes:\n",
    "        #         # Basic part\n",
    "        #         dict_obj = {\n",
    "        #             'class': self.opt.c,\n",
    "        #             'ct': box[4]['ct'],\n",
    "        #             'bbox': np.array(box[4]['bbox']).tolist(),\n",
    "        #             'confidence': box[4]['score'],\n",
    "        #             'kps_displacement_mean': box[4]['kps_displacement_mean'].tolist(),\n",
    "        #             'kps_heatmap_mean': box[4]['kps_heatmap_mean'].tolist(),\n",
    "\n",
    "        #             'kps_heatmap_std': box[4]['kps_heatmap_std'].tolist(),\n",
    "        #             'kps_heatmap_height': box[4]['kps_heatmap_height'].tolist(),\n",
    "        #             'obj_scale': box[4]['obj_scale'].tolist(),\n",
    "        #         }\n",
    "\n",
    "        #         # Optional part\n",
    "        #         if self.opt.use_pnp:\n",
    "        #             if 'location' in box[4]:\n",
    "        #                 dict_obj['location'] = box[4]['location']\n",
    "        #                 dict_obj['quaternion_xyzw'] = box[4]['quaternion_xyzw'].tolist()\n",
    "        #             if 'kps_pnp' in box[4]:\n",
    "        #                 dict_obj['kps_pnp'] = box[4]['kps_pnp'].tolist()\n",
    "        #                 dict_obj['kps_3d_cam'] = box[4]['kps_3d_cam'].tolist()\n",
    "\n",
    "        #         dict_out['objects'].append(dict_obj)\n",
    "\n",
    "\n",
    "        # if self.opt.debug >= 1 and self.opt.debug < 4:\n",
    "        #     self.show_results(debugger, image, results)\n",
    "\n",
    "        # # Saving path is specific for demo folder structure\n",
    "        # elif self.opt.debug == 4:\n",
    "        #     self.save_results(debugger, image, results, image_or_path_or_tensor, dict_out)\n",
    "\n",
    "        # # Saving path is specific for evaluation\n",
    "        # elif self.opt.debug == 6:\n",
    "        #     self.save_results_eval(debugger, image, results, image_or_path_or_tensor, dict_out)\n",
    "\n",
    "        # Save results for debug, boxes for evaluation, output is the original network output\n",
    "        # Todo: Actually results could be combined with boxes\n",
    "        return {'results': results, 'boxes': boxes, 'output': output, 'meta_inp' : meta_inp,'tot': tot_time, 'load': load_time,\n",
    "                'pre': pre_time, 'net': net_time, 'dec': dec_time,\n",
    "                'post': post_time, 'merge': merge_time, 'pnp': pnp_time, 'track': track_time}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_image_official import Evaluator\n",
    "\n",
    "class MyEvaluator(Evaluator):\n",
    "    def __init__(self, opt):\n",
    "        super().__init__(opt)\n",
    "        self.detector = MyObjectPoseDetector(self.opt)\n",
    "        \n",
    "    def predict(self, image):\n",
    "        image = cv2.cvtColor(image.copy(), cv2.COLOR_RGB2BGR)\n",
    "        meta = {\"image_shape\": image.shape}\n",
    "        \n",
    "        ret = self.detector.run(image, meta_inp=meta)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def evaluate(self, batch):\n",
    "        images, labels, projs, cam_intrinsics, planes, views = [], [], [], [], [], []\n",
    "        filenames = []\n",
    "\n",
    "        # 1\n",
    "        for image_path in batch[0]:\n",
    "            image = cv2.imread(image_path)\n",
    "            # image = cv2.imread('/home/CenterPose/images/CenterPose/chair/00000.png')\n",
    "\n",
    "            images.append(image)\n",
    "            \n",
    "        \n",
    "        for label in batch[1]:\n",
    "            labels.append(label)\n",
    "\n",
    "            # Save to check the number of views\n",
    "            # self.filename_list.append(filename)\n",
    "\n",
    "        # It can be incorporated into the next for block if we support batch processing.\n",
    "        # Since we use pnp here, not valid for now.\n",
    "        local_id = 0\n",
    "        results = []\n",
    "        # 2\n",
    "        for image, label in zip(images, labels):\n",
    "\n",
    "            local_id = local_id + 1\n",
    "            if self.NUM_SAMPLE % self.opt.batch_size == 0:\n",
    "                global_id = self.NUM_SAMPLE - self.opt.batch_size + local_id\n",
    "            else:\n",
    "                global_id = self.NUM_SAMPLE - self.NUM_SAMPLE % self.opt.batch_size + local_id\n",
    "\n",
    "            if type(self.opt.eval_skip) is list:\n",
    "                if global_id in self.opt.eval_skip:\n",
    "                    results.append([])\n",
    "                    continue\n",
    "            else:\n",
    "                if global_id < self.opt.eval_skip:\n",
    "                    results.append([])\n",
    "                    continue\n",
    "\n",
    "            if self.opt.eval_subset_list is not None and global_id not in self.opt.eval_subset_list:\n",
    "                results.append([])\n",
    "                continue\n",
    "\n",
    "            # # The camera intrinsics have to be updated\n",
    "            # cam_intrinsic[:2, :3] = cam_intrinsic[:2, :3] / self.opt.eval_resolution_ratio\n",
    "            # cx = cam_intrinsic[0, 2]\n",
    "            # cy = cam_intrinsic[1, 2]\n",
    "            # cam_intrinsic[0, 2] = cy\n",
    "            # cam_intrinsic[1, 2] = cx\n",
    "\n",
    "            # if self.opt.c == 'cup':\n",
    "            #     if all(label['MugFlag_instance']) == True:\n",
    "            #         results.append(self.predict(image, cam_intrinsic, projection_matrix, filename, global_id, True))\n",
    "            #     elif all(np.invert(label['MugFlag_instance'])) == True:\n",
    "            #         results.append(self.predict(image, cam_intrinsic, projection_matrix, filename, global_id, False))\n",
    "            #     else:\n",
    "            #         results.append(self.predict(image, cam_intrinsic, projection_matrix, filename, global_id, True))\n",
    "            #         # Todo: May assume that we already know it is cup or mug\n",
    "\n",
    "            # else:\n",
    "            #     results.append(self.predict(image, cam_intrinsic, projection_matrix, filename, global_id))\n",
    "    \n",
    "            results.append(self.predict(image))\n",
    "\n",
    "        local_id = 0\n",
    "        # 3\n",
    "        for ret, label in zip(results, labels):\n",
    "            boxes = ret['boxes']\n",
    "            meta_inp = ret['meta_inp']\n",
    "\n",
    "            local_id = local_id + 1\n",
    "            if self.NUM_SAMPLE % self.opt.batch_size == 0:\n",
    "                global_id = self.NUM_SAMPLE - self.opt.batch_size + local_id\n",
    "            else:\n",
    "                global_id = self.NUM_SAMPLE - self.NUM_SAMPLE % self.opt.batch_size + local_id\n",
    "\n",
    "            if type(self.opt.eval_skip) is list:\n",
    "                if global_id in self.opt.eval_skip:\n",
    "                    continue\n",
    "            else:\n",
    "                if global_id < self.opt.eval_skip:\n",
    "                    continue\n",
    "\n",
    "            if self.opt.eval_subset_list is not None and global_id not in self.opt.eval_subset_list:\n",
    "                continue\n",
    "\n",
    "            # # Extract gt info\n",
    "            # instances_scale = label['scale_instance']\n",
    "            print('label : \\n', label)\n",
    "            print('image_shape : \\n', meta_inp['image_shape'])\n",
    "            normalized_label = [[x / meta_inp['image_shape'][1], y / meta_inp['image_shape'][0]] for x, y in label]\n",
    "            print('normalized_label : \\n', normalized_label)\n",
    "            # instances_3d = label['3d_instance']\n",
    "            # instances_Mo2c = label['Mo2c_instance']\n",
    "            # if self.opt.c == 'cup':\n",
    "            #     instances_MugFlag = label['MugFlag_instance']\n",
    "\n",
    "            #     if self.opt.mug_only == True:\n",
    "            #         # Only count the case with mug\n",
    "            #         if all(np.invert(label['MugFlag_instance'])) == True:\n",
    "            #             continue\n",
    "\n",
    "\n",
    "            #     elif self.opt.mug_only == False:\n",
    "            #         # Only count the case with cup\n",
    "            #         if all(np.invert(label['MugFlag_instance'])) == False:\n",
    "            #             continue\n",
    "\n",
    "            # visibilities = label['visibility']\n",
    "\n",
    "            # num_instances = 0\n",
    "            # for instance, instance_3d, visibility in zip(\n",
    "            #         instances_2d, instances_3d, visibilities):\n",
    "            #     if (visibility > self._vis_thresh and\n",
    "            #             self._is_visible(instance[0]) and instance_3d[0, 2] < 0):\n",
    "            #         num_instances += 1\n",
    "\n",
    "            # # We don't have negative examples in evaluation.\n",
    "            # if num_instances == 0:\n",
    "            #     continue\n",
    "\n",
    "            # scale_hit_miss = metrics.HitMiss(self._scale_thresholds)\n",
    "            # iou_hit_miss = metrics.HitMiss(self._iou_thresholds)\n",
    "            # azimuth_hit_miss = metrics.HitMiss(self._azimuth_thresholds)\n",
    "            # polar_hit_miss = metrics.HitMiss(self._polar_thresholds)\n",
    "            # pixel_hit_miss = metrics.HitMiss(self._pixel_thresholds)\n",
    "            # add_hit_miss = metrics.HitMiss(self._add_thresholds)\n",
    "            # adds_hit_miss = metrics.HitMiss(self._adds_thresholds)\n",
    "\n",
    "            # # For debug\n",
    "            # pred_box_list = []\n",
    "            # gt_box_list = []\n",
    "\n",
    "            # # Save gt info for Stephen\n",
    "            # M_c2w = np.linalg.inv(view)\n",
    "            # dict_save = {\n",
    "            #     'filename': filename,\n",
    "            #     'camera_pose': M_c2w.tolist(),  # M_c2w\n",
    "            #     'camera_intrinsics': cam_intrinsic.tolist(),  # has been transformed to list\n",
    "            #     'image_id': int(label['image_id']),\n",
    "            #     \"objects\": [],\n",
    "            # }\n",
    "\n",
    "            num_matched = 0\n",
    "\n",
    "        \n",
    "            for idx_box, box in enumerate(boxes):\n",
    "\n",
    "                # Correspond to one prediction in one image\n",
    "                # box_point_2d, box_point_3d, relative_scale, box_point_2d_ori, result_ori = box\n",
    "                box_point_2d, relative_scale, box_point_2d_ori, result_ori = box\n",
    "\n",
    "                print(\"box_point_2d : \\n\", box_point_2d)\n",
    "                print(\"relative_scale : \\n\", relative_scale)\n",
    "                print(\"box_point_2d_ori : \\n\", box_point_2d_ori)\n",
    "                print(\"result_ori : \\n\", result_ori)\n",
    "                # Todo:\n",
    "                if self.opt.eval_MobilePose_postprocessing == True:\n",
    "                    box_point_2d, box_point_3d = self.Lift2DTo3D(projection_matrix, result_ori, image.shape[0],\n",
    "                                                                 image.shape[1])\n",
    "\n",
    "                index = self.match_box(box_point_2d, instances_2d, visibilities)\n",
    "                if index >= 0:\n",
    "                    num_matched += 1\n",
    "\n",
    "                    # Apply gt_scale to recalculate pnp\n",
    "                    if self.opt.eval_gt_scale == True:\n",
    "                        result_gt_scale = self.predict_gt_scale(result_ori, instances_scale[index], cam_intrinsic)\n",
    "                        if result_gt_scale is not None:\n",
    "                            box_point_2d, box_point_3d, _, _, _ = result_gt_scale\n",
    "\n",
    "                    # If you only compute the 3D bounding boxes from RGB images,\n",
    "                    # your 3D keypoints may be upto scale. However the ground truth\n",
    "                    # is at metric scale. There is a hack to re-scale your box using\n",
    "                    # the ground planes (assuming your box is sitting on the ground).\n",
    "                    # However many models learn to predict depths and scale correctly.\n",
    "\n",
    "                    if not self.opt.use_absolute_scale:\n",
    "                        scale = self.compute_scale(box_point_3d, plane)\n",
    "                        box_point_3d = box_point_3d * scale\n",
    "                        boxes[idx_box] = list(boxes[idx_box])\n",
    "                        boxes[idx_box].append(box_point_3d)\n",
    "\n",
    "                    print(f'Sample {global_id}')\n",
    "                    print(f'GT: {instances_scale[index] / instances_scale[index][1]}')\n",
    "                    print(f'Pred: {relative_scale / relative_scale[1]}')\n",
    "                    if self.opt.c == 'cup':\n",
    "                        pixel_error = self.evaluate_2d(box_point_2d, instances_2d[index], instances_3d[index],\n",
    "                                                       instances_Mo2c[index], projection_matrix,\n",
    "                                                       instances_MugFlag[index])\n",
    "                        azimuth_error, polar_error, iou, pred_box, gt_box, add, adds = self.evaluate_3d(box_point_3d,\n",
    "                                                                                                        instances_3d[\n",
    "                                                                                                            index],\n",
    "                                                                                                        instances_Mo2c[\n",
    "                                                                                                            index],\n",
    "                                                                                                        instances_MugFlag[\n",
    "                                                                                                            index])\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        pixel_error = self.evaluate_2d(box_point_2d, instances_2d[index], instances_3d[index],\n",
    "                                                       instances_Mo2c[index], projection_matrix)\n",
    "                        azimuth_error, polar_error, iou, pred_box, gt_box, add, adds = self.evaluate_3d(box_point_3d,\n",
    "                                                                                                        instances_3d[\n",
    "                                                                                                            index],\n",
    "                                                                                                        instances_Mo2c[\n",
    "                                                                                                            index])\n",
    "\n",
    "                    # Record some predictions\n",
    "                    M_o2w = M_c2w @ instances_Mo2c[index]\n",
    "                    instances_3d_w = M_c2w @ np.hstack(\n",
    "                        (instances_3d[index], np.ones((instances_3d[index].shape[0], 1)))).T\n",
    "\n",
    "                    instances_3d_w = instances_3d_w[:3, :].T\n",
    "\n",
    "                    keypoint_2d_gt = [np.multiply(keypoint, np.asarray([self.width, self.height], np.float32)) for\n",
    "                                      keypoint in instances_2d[index]]\n",
    "\n",
    "                    result_pnp = [np.multiply(keypoint, np.asarray([self.width, self.height], np.float32)) for\n",
    "                                  keypoint in box_point_2d]\n",
    "\n",
    "                    scale_error = self.evaluate_scale(relative_scale, instances_scale[index])\n",
    "\n",
    "                    print(f'Scale_error: {scale_error}')\n",
    "                    print('\\n')\n",
    "\n",
    "                    dict_obj = {\n",
    "                        'class': self.opt.c,\n",
    "                        'keypoint_2d_pred_displacement': np.array(result_ori['kps_displacement_mean']).reshape(1,\n",
    "                                                                                                               -1).tolist(),\n",
    "                        'keypoint_2d_pred_heatmap': np.array(result_ori['kps_heatmap_mean']).reshape(1, -1).tolist(),\n",
    "                        'keypoint_2d_pred_pnp': np.array(result_pnp).reshape(1, -1).tolist(),\n",
    "                        'keypoint_2d_gt': np.array(keypoint_2d_gt).reshape(1, -1).tolist(),\n",
    "                        'relative_scale': relative_scale.tolist(),\n",
    "                        'object_pose_gt_w': M_o2w.tolist(),  # 4x4 matrix\n",
    "                        'keypoint_3d_gt_w': instances_3d_w.tolist(),  # 9x3 array\n",
    "                        'keypoint_3d_pred_unscaled_c': np.array(boxes[idx_box][1]).reshape(1, -1).tolist(),  # 27 list\n",
    "                        'keypoint_3d_pred_scaled_c': np.array(boxes[idx_box][5]).reshape(1, -1).tolist(),  # 27 list\n",
    "                        '3DIoU': iou,\n",
    "                        'error_2Dpixel': pixel_error,\n",
    "                        'error_azimuth': azimuth_error,\n",
    "                        'error_polar_error': polar_error,\n",
    "                        'error_scale': scale_error\n",
    "                    }\n",
    "                    dict_save['objects'].append(dict_obj)\n",
    "\n",
    "                    pred_box_list.append(pred_box)\n",
    "                    gt_box_list.append(gt_box)\n",
    "\n",
    "                    conf = result_ori['score']\n",
    "\n",
    "                else:\n",
    "                    conf = 0\n",
    "                    pixel_error = _MAX_PIXEL_ERROR\n",
    "                    azimuth_error = _MAX_AZIMUTH_ERROR\n",
    "                    polar_error = _MAX_POLAR_ERROR\n",
    "                    iou = 0.\n",
    "                    add = _MAX_DISTANCE\n",
    "                    adds = _MAX_DISTANCE\n",
    "                    scale_error = _MAX_SCALE_ERROR\n",
    "\n",
    "                if METRIC_UPDATED:\n",
    "                    # New\n",
    "                    scale_hit_miss.record_hit_miss([scale_error, conf], greater=False)\n",
    "                    iou_hit_miss.record_hit_miss([iou, conf])\n",
    "                    add_hit_miss.record_hit_miss([add, conf], greater=False)\n",
    "                    adds_hit_miss.record_hit_miss([adds, conf], greater=False)\n",
    "                    pixel_hit_miss.record_hit_miss([pixel_error, conf], greater=False)\n",
    "                    azimuth_hit_miss.record_hit_miss([azimuth_error, conf], greater=False)\n",
    "                    polar_hit_miss.record_hit_miss([polar_error, conf], greater=False)\n",
    "                else:\n",
    "                    # Old\n",
    "                    scale_hit_miss.record_hit_miss(scale_error, greater=False)\n",
    "                    iou_hit_miss.record_hit_miss(iou)\n",
    "                    add_hit_miss.record_hit_miss(add, greater=False)\n",
    "                    adds_hit_miss.record_hit_miss(adds, greater=False)\n",
    "                    pixel_hit_miss.record_hit_miss(pixel_error, greater=False)\n",
    "                    azimuth_hit_miss.record_hit_miss(azimuth_error, greater=False)\n",
    "                    polar_hit_miss.record_hit_miss(polar_error, greater=False)\n",
    "\n",
    "            if self.opt.eval_debug_json == True:\n",
    "                json_filename = f'{self.opt.outf}/{self.opt.c}_{self.opt.eval_save_id}/{filename}_{global_id}_record.json'\n",
    "                with open(json_filename, 'w+') as fp:\n",
    "                    json.dump(dict_save, fp, indent=4, sort_keys=True)\n",
    "\n",
    "            # For debug\n",
    "            if self.opt.eval_debug == True:\n",
    "                # if self.opt.eval_debug == True and iou<self.opt.eval_debug_save_thresh:\n",
    "                self.debug(image.copy(), num_instances, instances_2d, instances_3d, projection_matrix, boxes,\n",
    "                           instances_scale, filename, pred_box_list, gt_box_list, global_id)\n",
    "\n",
    "            self._scale_ap.append(scale_hit_miss, len(instances_2d))\n",
    "            self._iou_ap.append(iou_hit_miss, len(instances_2d))\n",
    "            self._pixel_ap.append(pixel_hit_miss, len(instances_2d))\n",
    "            self._azimuth_ap.append(azimuth_hit_miss, len(instances_2d))\n",
    "            self._polar_ap.append(polar_hit_miss, len(instances_2d))\n",
    "            self._add_ap.append(add_hit_miss, len(instances_2d))\n",
    "            self._adds_ap.append(adds_hit_miss, len(instances_2d))\n",
    "            self._matched += num_matched\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ../../../models/CenterPose/chair_v1_140.pth, epoch 140\n",
      "Creating model...\n",
      "loaded ../../../models/CenterPose/chair_v1_140.pth, epoch 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label : \n",
      " [[1237, 559], [1086, 551], [1344, 599], [1332, 687], [1086, 633], [1160, 431], [1389, 464], [1385, 533], [1158, 494]]\n",
      "image_shape : \n",
      " (1080, 1920, 3)\n",
      "normalized_label : \n",
      " [[0.6442708333333333, 0.5175925925925926], [0.565625, 0.5101851851851852], [0.7, 0.5546296296296296], [0.69375, 0.6361111111111111], [0.565625, 0.5861111111111111], [0.6041666666666666, 0.3990740740740741], [0.7234375, 0.42962962962962964], [0.7213541666666666, 0.4935185185185185], [0.603125, 0.45740740740740743]]\n",
      "box_point_2d : \n",
      " [array([0.12011461, 0.62818146]), array([0.27649432, 0.7273408 ]), array([0.02368109, 0.20974551]), array([0.18852365, 0.22672388]), array([0.22894239, 0.4559918 ]), array([0.37535554, 0.49913502]), array([0.16074832, 0.06825723]), array([0.32991973, 0.05733914])]\n",
      "relative_scale : \n",
      " [0.7418223  1.0003369  0.76515776]\n",
      "box_point_2d_ori : \n",
      " [[0.21055368 0.35766687]\n",
      " [0.12499993 0.61111079]\n",
      " [0.28906253 0.69444434]\n",
      " [0.02368109 0.20974551]\n",
      " [0.20312494 0.22222201]\n",
      " [0.21093756 0.44444397]\n",
      " [0.35156262 0.52777778]\n",
      " [0.16074832 0.06825723]\n",
      " [0.32031244 0.08333333]]\n",
      "result_ori : \n",
      " {'score': 0.9151394367218018, 'cls': 0, 'obj_scale': array([0.7418223 , 1.0003369 , 0.76515776], dtype=float32), 'obj_scale_uncertainty': array([0., 0., 0.], dtype=float32), 'kps_displacement_std': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), 'bbox': [53.646554946899414, 79.25697326660156, 708.7403869628906, 775.0439071655273], 'ct': [381.193470954895, 427.15044021606445], 'kps': array([239.99987125, 659.99965668, 555.00005722, 749.99988556,\n",
      "        45.46769142, 226.52515411, 389.99988556, 239.99977112,\n",
      "       405.00011444, 479.99948502, 675.00022888, 570.        ,\n",
      "       308.63677025,  73.71780396, 614.99988556,  90.        ]), 'tracking': array([0., 0.], dtype=float32), 'tracking_hp': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), 'kps_displacement_mean': array([230.62005043, 678.43597412, 530.86910248, 785.52806854,\n",
      "        45.46769142, 226.52515411, 361.96540833, 244.86179352,\n",
      "       439.56939697, 492.47114182, 720.68264008, 539.06581879,\n",
      "       308.63677025,  73.71780396, 633.44587326,  61.92626953]), 'kps_heatmap_mean': array([-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "       -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "       -10000., -10000.]), 'kps_heatmap_std': array([-48000., -48000., -48000., -48000., -48000., -48000., -48000.,\n",
      "       -48000., -48000., -48000., -48000., -48000., -48000., -48000.,\n",
      "       -48000., -48000.], dtype=float32), 'kps_heatmap_height': array([-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "       -10000.], dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'instances_2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataloader):\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# print(batch)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# For debug\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     evaluator\u001b[38;5;241m.\u001b[39mNUM_SAMPLE \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mNUM_SAMPLE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# if evaluator.NUM_SAMPLE in [1000,5000,10000,20000,50000,100000]:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#     evaluator.finalize()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#     evaluator.write_report( report_file= os.path.splitext(opt.report_file)[0] + f'_{evaluator.NUM_SAMPLE}.txt')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[89], line 178\u001b[0m, in \u001b[0;36mMyEvaluator.evaluate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39meval_MobilePose_postprocessing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     box_point_2d, box_point_3d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLift2DTo3D(projection_matrix, result_ori, image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    176\u001b[0m                                                  image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 178\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_box(box_point_2d, \u001b[43minstances_2d\u001b[49m, visibilities)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    180\u001b[0m     num_matched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'instances_2d' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "evaluator = MyEvaluator(opt_combined)\n",
    "# objectron_buckett = 'gs://objectron/v1/records_shuffled'\n",
    "# eval_shards = tf.io.gfile.glob(objectron_buckett + opt.eval_data)\n",
    "# ds = tf.data.TFRecordDataset(eval_shards).take(opt.eval_max_num)\n",
    "def custom_collate_fn(batch):\n",
    "    # batch는 [(image1, dict1), (image2, dict2), ...] 형식\n",
    "    images = [item[0] for item in batch]\n",
    "    dicts = [item[1] for item in batch]\n",
    "    \n",
    "    return images, dicts\n",
    "\n",
    "# DataLoader 설정\n",
    "dataloader = DataLoader(my_dataset, batch_size=opt_combined.batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "batch = []\n",
    "for batch in tqdm.tqdm(dataloader):\n",
    "\n",
    "    # print(batch)\n",
    "    # print(len(batch))\n",
    "    # print(len(batch[0]))\n",
    "\n",
    "    \n",
    "    # For debug\n",
    "    evaluator.NUM_SAMPLE = evaluator.NUM_SAMPLE + 1\n",
    "\n",
    "    evaluator.evaluate(batch)\n",
    "\n",
    "    break\n",
    "    # if evaluator.NUM_SAMPLE in [1000,5000,10000,20000,50000,100000]:\n",
    "    #     evaluator.finalize()\n",
    "    #     evaluator.write_report( report_file= os.path.splitext(opt.report_file)[0] + f'_{evaluator.NUM_SAMPLE}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CenterPose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
